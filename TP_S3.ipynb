{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP de NLP sur la Séance 3 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliothèques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/Cornell-University/movie-dialog-corpus"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/8.91M [00:00<?, ?B/s]\n",
      " 11%|█         | 1.00M/8.91M [00:00<00:04, 1.98MB/s]\n",
      " 34%|███▎      | 3.00M/8.91M [00:00<00:01, 5.41MB/s]\n",
      " 56%|█████▌    | 5.00M/8.91M [00:00<00:00, 8.17MB/s]\n",
      " 79%|███████▊  | 7.00M/8.91M [00:00<00:00, 9.61MB/s]\n",
      "100%|██████████| 8.91M/8.91M [00:01<00:00, 10.4MB/s]\n",
      "100%|██████████| 8.91M/8.91M [00:01<00:00, 8.13MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "License(s): other\n",
      "Downloading movie-dialog-corpus.zip to c:\\Users\\rymkm\\Desktop\\Cours 2023 - 2024\\IA School M2\\S2\\S2.2\\NLP\\Séance 3 TP\\NLP_TP_S-ance_3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Installation \n",
    "\n",
    "#!pip install kaggle\n",
    "#!kaggle datasets download -d Cornell-University/movie-dialog-corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des bibliothèques \n",
    "\n",
    "#traitement du csv\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Traitement du NLP\n",
    "import spacy\n",
    "\n",
    "# Création du modèle word to vect \n",
    "import gensim\n",
    "\n",
    "#bibliothèques usuelles\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PCA \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#Ajout d'une barre d'avancement\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration des classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir une classe\n",
    "class Corpus(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "    \n",
    "    # méthode qui itère sur les instances de la classe\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, \"r\") as i:\n",
    "            reader = csv.reader(i, delimiter=\",\")\n",
    "            for _, abstract in reader:\n",
    "                tokens = [t.text.lower() for t in self.nlp(abstract)]\n",
    "                yield tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import traitement et décompression du fichier zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#À compiler dans le bash \n",
    "\n",
    "#mkdir data/movie-dialog-corpus/\n",
    "#unzip -o movie-dialog-corpus.zip -d data/movie-dialog-corpus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convertir le tsv en csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du répertoire contenant les fichiers TSV\n",
    "tsv_directory = \"data/movie-dialog-corpus/\"\n",
    "# Chemin du répertoire où les fichiers CSV convertis seront enregistrés\n",
    "csv_directory = \"data/movie-dialog-csv/\"\n",
    "\n",
    "# Création du répertoire de destination s'il n'existe pas\n",
    "os.makedirs(csv_directory, exist_ok=True)\n",
    "\n",
    "# Parcours de tous les fichiers dans le répertoire TSV\n",
    "for file_name in os.listdir(tsv_directory):\n",
    "    if file_name.endswith('.tsv'):\n",
    "        # Chemin complet du fichier TSV\n",
    "        tsv_file_path = os.path.join(tsv_directory, file_name)\n",
    "        # Chemin complet du fichier CSV de destination\n",
    "        csv_file_path = os.path.join(csv_directory, os.path.splitext(file_name)[0] + '.csv')\n",
    "\n",
    "        # Lecture du fichier TSV et écriture dans le fichier CSV\n",
    "        with open(tsv_file_path, 'r', encoding='utf-8') as tsv_file, \\\n",
    "             open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "\n",
    "            # Création d'un objet lecteur TSV\n",
    "            tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "            # Création d'un objet écrivain CSV\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "\n",
    "            # Copie des lignes du fichier TSV dans le fichier CSV\n",
    "            for row in tsv_reader:\n",
    "                csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = Corpus(\"data/movie-dialog-csv/movie_conversations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/movie-dialog-csv/movie_conversations.csv\",'r',encoding='utf-8') as file:\n",
    "    documents = [line.strip().split() for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement des word embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    "import spacy\n",
    "\n",
    "# Chargement du modèle SpaCy pour le traitement du langage naturel\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Liste pour stocker les phrases prétraitées\n",
    "preprocessed_sentences = []\n",
    "\n",
    "# Chargement des données à partir du fichier CSV\n",
    "with open(\"data/movie-dialog-csv/movie_conversations.csv\", 'r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        # Prétraitement de chaque ligne du fichier CSV\n",
    "        # (vous pouvez personnaliser cette étape en fonction de vos besoins)\n",
    "        tokens = [token.text.lower() for token in nlp(row[0])]\n",
    "        preprocessed_sentences.append(tokens)\n",
    "\n",
    "# Construction et entraînement du modèle Word2Vec\n",
    "model = Word2Vec(sentences=preprocessed_sentences,\n",
    "                 min_count=100,  # Ignorer les mots ayant une fréquence inférieure à 100\n",
    "                 window=5,       # Taille de la fenêtre contextuelle\n",
    "                 vector_size=100)  # Dimension des vecteurs de mots\n",
    "\n",
    "# Vous pouvez maintenant utiliser le modèle pour effectuer des tâches de traitement du langage naturel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation des phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Transformer' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mwv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key_or_keys)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[1;32mc:\\Users\\rymkm\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'Transformer' not present\""
     ]
    }
   ],
   "source": [
    "model.wv['Transformer'] # afficher le vecteur du mot 'language'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des similarités "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.similarity('Transformer', 'language')) # 100% de similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similar_by_word('NMT',topn=15) # afficher les mots similaires à 'language'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.doesnt_match(\"bert word2vec GPT Transformer\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation du word embedding dans l'espace \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE # importer TSNE pour visualiser les vecteurs dans un espace 2D\n",
    "\n",
    "#on choisis un mot et on affiche les mots similaires\n",
    "target_word = 'BERT'\n",
    "similar_words = [word for word, _ in model.wv.most_similar(target_word, topn=200)] + [target_word] # on prend les 20 mots les plus similaires à 'BERT' et on ajoute 'BERT' à la liste\n",
    "\n",
    "# on fait le embedding des mots similaires\n",
    "embeddings = [model.wv[word] for word in similar_words] + model.wv[target_word] \n",
    "# on ajoute le vecteur de 'BERT' à la liste des vecteurs des mots similaires\n",
    "mapped_embeddings = TSNE(n_components=2, # on veut réduire la dimensionalité à 2D\n",
    "                         metric='cosine', # on utilise la distance cosinus pour mesurer la similarité entre les vecteurs\n",
    "                         init='pca', # on initialise les vecteurs avec PCA\n",
    "                         perplexity=10, # on définit la perplexité à 10 pour que les points soient bien répartis dans l'espace, 10 veut dire qu'on regarde les 10 plus proches voisins\n",
    "                         ).fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des résultats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20)) # on définit la taille de la figure\n",
    "x = mapped_embeddings[:, 0] # on prend la première colonne de la matrice\n",
    "y = mapped_embeddings[:, 1] # on prend la deuxième colonne de la matrice\n",
    "plt.scatter(x, y) # on affiche les points\n",
    "for i, word in enumerate(similar_words): # on affiche les mots similaires à 'BERT'\n",
    "    plt.annotate(word, (x[i], y[i]), fontsize=12) # on affiche le mot à la position correspondante\n",
    "plt.show() # on affiche la figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des résultats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # importer tqdm pour afficher une barre de progression\n",
    "\n",
    "nlp= spacy.load('en_core_web_sm') # charger le modèle de langue anglaise\n",
    "\n",
    "wordic= {} # initialiser un dictionnaire vide\n",
    "for word in tqdm(model.wv.index_to_key): # pour chaque mot dans le vocabulaire du modèle Word2Vec\n",
    "    wordic[word]= nlp(word)[0].pos_ # ajouter le mot et son vocabulaire dans le dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordic['translation']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
